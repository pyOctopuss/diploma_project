{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a34bbce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf4c9092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import yaml\n",
    "import itertools\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cbc0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e942dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(company, period, models_list):\n",
    "    \n",
    "    '''\n",
    "    Find paths to result (dir /wf_result/) files of experiments\n",
    "    '''\n",
    "\n",
    "    uuids = []\n",
    "    model_names = []\n",
    "    train_start_or_duration = []\n",
    "    hyperparameters = []\n",
    "    features = []\n",
    "    \n",
    "    file_paths_to_pred = []\n",
    "    file_paths_to_info = []\n",
    "\n",
    "    for model in models_list:\n",
    "        basic_path = '/diploma_info/datalake/'\n",
    "        path_to_res = f'{company}/{period}/{model}'\n",
    "\n",
    "        file_paths_to_pred += glob(max(glob(basic_path+f'forecast/regression/{path_to_res}/research_task_*'), key=os.path.getctime) + f'/{model}_*/forecast_*.csv')\n",
    "        file_paths_to_info += glob(max(glob(basic_path+f'wf_result/regression/{path_to_res}/research_task_*'), key=os.path.getctime) + f'/{model}_*.yaml')\n",
    "            \n",
    "    print(len(file_paths_to_info), len(file_paths_to_pred))\n",
    "\n",
    "\n",
    "    for file in file_paths_to_info: \n",
    "        with open(file, 'r') as f:\n",
    "            res = yaml.safe_load(f)\n",
    "        uuids.append(res['unique_uuid'])\n",
    "        model_names.append(res['model_name'])\n",
    "        hyperparameters.append(res['model_hyperparameters'])\n",
    "        features.append(res['train_features'])\n",
    "        if 'duration_training_history' in res:\n",
    "            train_start_or_duration.append(res['duration_training_history'])\n",
    "        else:\n",
    "            train_start_or_duration.append(res['train_start'])\n",
    "            \n",
    "\n",
    "    shorten_uuids = [n.split('-')[-2] for n in uuids]\n",
    "\n",
    "    model_parameters = list(zip(uuids, shorten_uuids, model_names, train_start_or_duration, hyperparameters, features))           \n",
    "\n",
    "\n",
    "    return file_paths_to_pred, file_paths_to_info, model_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb6b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preds(company, period, test_start, test_end, file_paths_to_pred):\n",
    "    \n",
    "    '''\n",
    "    Makes dataframe with true values and predictions\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    dateparse = lambda dates: datetime.strptime(dates, '%Y-%m-%d')\n",
    "    path_to_full_set = f'/diploma_info/datalake/processed_data/{company}_{period}.csv'\n",
    "\n",
    "    full_set = pd.read_csv(\n",
    "        path_to_full_set,\n",
    "        parse_dates=['date'],\n",
    "        index_col='date', \n",
    "        date_parser=dateparse\n",
    "    )\n",
    "    target = full_set.loc[:, ['close']]\n",
    "    \n",
    "    \n",
    "    df_preds = target.copy()\n",
    "    \n",
    "    for num_exp, exp_paths in enumerate(file_paths_to_pred):\n",
    "    \n",
    "        if len(exp_paths.split('_')) == 7:\n",
    "            exp_name = exp_paths.split('.')[0].split('_')[-1]+'_'+exp_paths.split('-')[-3]+'_'+exp_paths.split('_')[-2]\n",
    "        elif len(exp_paths.split('_')) == 10:\n",
    "            exp_name = \"_\".join(exp_paths.split('.')[0].split('_')[-2:])+'_'+exp_paths.split('-')[-3]+'_'+exp_paths.split('_')[-3]\n",
    "            \n",
    "#         print(exp_name)\n",
    "        \n",
    "        pred = pd.read_csv(\n",
    "            exp_paths,\n",
    "            parse_dates=['date_time'],\n",
    "            index_col='date_time', \n",
    "            date_parser=dateparse\n",
    "        )\n",
    "        pred.index.name = 'date'\n",
    "        pred.columns = [f'{exp_name}']\n",
    "        \n",
    "\n",
    "        for date in pred.index:\n",
    "            df_preds.loc[date, f'{exp_name}'] = pred.loc[date, f'{exp_name}']\n",
    "                   \n",
    "    df_preds = df_preds.loc[test_start:test_end]\n",
    "#     .dropna()\n",
    "#     print(df_preds)\n",
    "    \n",
    "    return df_preds, df_preds.shape[0], df_preds.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdbaaade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors_dataframe(df_preds, model_parameters):\n",
    "    \n",
    "    '''\n",
    "    Counts accuracy value of each tested model on each day\n",
    "    \n",
    "    '''    \n",
    "\n",
    "    uuids = []\n",
    "    model_names = []\n",
    "    train_start_or_duration = []\n",
    "    hyperparameters = []\n",
    "    features = []\n",
    "    \n",
    "    models = df_preds.columns[1:]\n",
    "\n",
    "    shorten_best_model_uuid = [n.split('_')[-2] for n in models]\n",
    "    \n",
    "    for m in shorten_best_model_uuid:  \n",
    "        for n in model_parameters:\n",
    "            if m==n[1]:\n",
    "                uuids.append(n[0])\n",
    "                model_names.append(n[2])\n",
    "                train_start_or_duration.append(n[-3])\n",
    "                hyperparameters.append(n[-2])\n",
    "                features.append(n[-1])\n",
    "    \n",
    "    stats_df = pd.DataFrame()\n",
    "    \n",
    "    for num, exp_name in enumerate(models):\n",
    "        \n",
    "        pred = df_preds[exp_name].dropna()\n",
    "        orig = df_preds.loc[pred.index, 'close'].values\n",
    "\n",
    "        abs_err_df = pd.DataFrame(np.around(orig-pred.values, 2))\n",
    "        rel_err_df = pd.DataFrame(np.around(abs(pred.values / orig - 1) * 100, 2))\n",
    "        \n",
    "        len_df_pred = len(pred.values)\n",
    "        n_less_5 = rel_err_df[rel_err_df.iloc[:, 0] < 5].shape[0]\n",
    "        \n",
    "        stats_df.loc[exp_name, 'forecasted_day'] = int(exp_name[-1])\n",
    "        stats_df.loc[exp_name, 'n_days_forecasted'] = len_df_pred\n",
    "        stats_df.loc[exp_name, 'n_days_err_less_5'] = n_less_5\n",
    "        stats_df.loc[exp_name, '%_days_err_less_5'] = round((n_less_5 / len_df_pred) * 100, 2)\n",
    "        stats_df.loc[exp_name, 'accuracy'] = np.around(100 - rel_err_df.mean().values, 4)\n",
    "        \n",
    "        stats_df.loc[exp_name, 'mean_abs_err'] = np.around(abs_err_df.mean().values, 4)\n",
    "        stats_df.loc[exp_name, 'mean_rel_err'] = np.around(rel_err_df.mean().values, 4)\n",
    "\n",
    "        stats_df.loc[exp_name, 'median_abs_err'] = np.around(abs_err_df.median().values, 4)\n",
    "        stats_df.loc[exp_name, 'median_rel_err'] = np.around(rel_err_df.median().values, 4)\n",
    "\n",
    "        stats_df.loc[exp_name, 'q25_abs_err'] = np.around(abs_err_df.quantile(0.25).values, 4)\n",
    "        stats_df.loc[exp_name, 'q25_rel_err'] = np.around(rel_err_df.quantile(0.25).values, 4)\n",
    "\n",
    "        stats_df.loc[exp_name, 'q75_abs_err'] = np.around(abs_err_df.quantile(0.75).values, 4)\n",
    "        stats_df.loc[exp_name, 'q75_rel_err'] = np.around(rel_err_df.quantile(0.75).values, 4)\n",
    "\n",
    "        stats_df.loc[exp_name, 'max_abs_err'] = np.around(abs_err_df.max().values, 4)\n",
    "        stats_df.loc[exp_name, 'max_rel_err'] = np.around(rel_err_df.max().values, 4)\n",
    "        \n",
    "        \n",
    "        stats_df.loc[exp_name, 'model'] = model_names[num]\n",
    "        stats_df.loc[exp_name, 'train_start/duration'] = train_start_or_duration[num]\n",
    "        stats_df.loc[exp_name, 'features'] = f\"{features[num]}\"\n",
    "        stats_df.loc[exp_name, 'hyperparameters'] = f\"{hyperparameters[num]}\"\n",
    "        stats_df.loc[exp_name, 'experiment_uuid'] = uuids[num]\n",
    "        \n",
    "    \n",
    "    \n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d5f330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMAZON daily\n",
      "36 180\n",
      "APPLE daily\n",
      "36 180\n",
      "GOOGLE daily\n",
      "36 180\n",
      "META daily\n",
      "36 180\n",
      "NETFLIX daily\n",
      "36 180\n"
     ]
    }
   ],
   "source": [
    "companies = [\"AMAZON\", \"APPLE\", \"GOOGLE\", \"META\", \"NETFLIX\"]\n",
    "time_period = [\"daily\"]\n",
    "# time_period = [\"daily\", \"weekly\", \"monthly\"]\n",
    "\n",
    "models_list = ['xgboost', 'lightgbm', 'random_forest', 'linear_regression']\n",
    "\n",
    "test_start = '2024-01-01'\n",
    "test_end = '2024-02-08'\n",
    "\n",
    "\n",
    "for company, period in list(itertools.product(companies, time_period)):\n",
    "    \n",
    "    print(company, period)\n",
    "    \n",
    "#     try:\n",
    "        \n",
    "    file_paths_to_pred, file_paths_to_info, model_parameters = get_paths(company.lower(), period, models_list)\n",
    "\n",
    "    df_preds, n_rows, n_cols = load_preds(company, period, test_start, test_end, file_paths_to_pred)\n",
    "\n",
    "    stats = get_errors_dataframe(df_preds, model_parameters)\n",
    "\n",
    "    stats.to_csv(os.path.join(f'/diploma_info/datalake/statistical_result/regression', \n",
    "                              f'statistical_result_{company.lower()}_{period}_{datetime.now().strftime(\"%Y%m%d\")}.csv'))\n",
    "    \n",
    "#     except ValueError as e:\n",
    "#         print(e)\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e1831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd58be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
